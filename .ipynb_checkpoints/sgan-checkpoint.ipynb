{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuS3rzb9E1PR"
   },
   "outputs": [],
   "source": [
    "# Lets start by loading the necessary libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import os\n",
    "# Use second GPU -- change if you want to use a first one\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "alPOj6Q8XaXn",
    "outputId": "7fbc10db-3cf4-4241-9d56-55f0125a8cda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CDExqVgZL9Ub",
    "outputId": "276f83a0-6d61-4355-a801-fcc72e91ce00"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "etvsPu4rMIli",
    "outputId": "5e803560-616b-4063-8e6d-6ef403fcfc0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading Fashion MNIST...\n",
      "trainset shape: (60000, 28, 28)\n",
      "testset shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "print(\"[INFO] loading Fashion MNIST...\")\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "\n",
    "\n",
    "print(\"trainset shape:\", trainX.shape)\n",
    "print(\"testset shape:\", testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tToTdVmTQzhC"
   },
   "outputs": [],
   "source": [
    "trainX = trainX.reshape(-1,28,28,1)\n",
    "testX = trainX.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7YzclzuRQyiO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    # scale to (0, 1)\n",
    "    x = ((x - x.min())/(255 - x.min()))\n",
    "    \n",
    "    # scale to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BrqMY6zRoFY"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, trainX,trainY, testX,testY, shuffle=True, scale_func=None):\n",
    "        self.train_x, self.train_y = trainX, trainY\n",
    "\n",
    "        self.test_x, self.test_y = testX, testY\n",
    "\n",
    "        # The mnist dataset comes with lots of labels, but for the purpose of this exercise,\n",
    "        # we will pretend that there are only 100.\n",
    "        # We use this mask to say which labels we will allow ourselves to use.\n",
    "        self.label_mask = np.zeros_like(self.train_y)\n",
    "        self.label_mask[0:1000] = 1\n",
    "        \n",
    "        # Roll the specified axis backwards, until it lies in a given position.\n",
    "        # From (32, 32, 3, 73257) to (73257, 32, 32, 3)\n",
    "        self.train_x = np.rollaxis(self.train_x, axis=3)\n",
    "        self.test_x = np.rollaxis(self.test_x, axis=3)\n",
    "\n",
    "        if scale_func is None:\n",
    "            self.scaler = scale\n",
    "        else:\n",
    "            self.scaler = scale_func\n",
    "        self.train_x = self.scaler(self.train_x)\n",
    "        \n",
    "        self.test_x = self.scaler(self.test_x)\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def batches(self, batch_size, which_set=\"train\"):\n",
    "        x_name = which_set + \"_x\"\n",
    "        y_name = which_set + \"_y\"\n",
    "\n",
    "        # Return the value of the named attribute of object\n",
    "        num_examples = len(dataset)\n",
    "        if self.shuffle:\n",
    "            idx = np.arange(num_examples)\n",
    "            np.random.shuffle(idx)\n",
    "            \n",
    "            dataset.train_x = dataset.train_x[idx]\n",
    "            setattr(dataset, x_name, getattr(dataset, x_name)[idx])\n",
    "            setattr(dataset, y_name, getattr(dataset, y_name)[idx])\n",
    "            if which_set == \"train\":\n",
    "                dataset.label_mask = dataset.label_mask[idx]\n",
    "        \n",
    "        dataset_x = getattr(dataset, x_name)\n",
    "        dataset_y = getattr(dataset, y_name)\n",
    "        for ii in range(0, num_examples, batch_size):\n",
    "            x = dataset_x[ii:ii+batch_size]\n",
    "            y = dataset_y[ii:ii+batch_size]\n",
    "            \n",
    "            if which_set == \"train\":\n",
    "                # When we use the data for training, we need to include\n",
    "                # the label mask, so we can pretend we don't have access\n",
    "                # to some of the labels, as an exercise of our semi-supervised\n",
    "                # learning ability\n",
    "                # x: [BATCH_SIZE, 32, 32, 3]\n",
    "                # y: [BATCH_SIZE, 1]\n",
    "                # label_mask: [BATCH_SIZE, 1] whether a 0: Label Cannot be used or 1: Label can be used \n",
    "                yield x, y, self.label_mask[ii:ii+batch_size]\n",
    "            else:\n",
    "                yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBj3BLLSSU07"
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    # placeholder for inputing the real images from the training set to the discriminator \n",
    "    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')\n",
    "    \n",
    "    # placeholder for inputing random noise data into the discriminator\n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    \n",
    "    # placeholder for inputing the real data labels (0-9 for the SVHN dataset)\n",
    "    y = tf.placeholder(tf.int32, (None), name='y')\n",
    "    \n",
    "    # placeholder for inputing the label masks which tell the model for which \n",
    "    # sample images should it take the labels for training\n",
    "    label_mask = tf.placeholder(tf.int32, (None), name='label_mask')\n",
    "    \n",
    "    return inputs_real, inputs_z, y, label_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwuYA_U5SYNu"
   },
   "outputs": [],
   "source": [
    "def generator(z, output_dim, reuse=False, alpha=0.2, training=True, size_mult=128):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        x1 = tf.layers.dense(z, 4 * 4 * size_mult * 4)\n",
    "        # Reshape it to start the convolutional stack\n",
    "        x1 = tf.reshape(x1, (-1, 4, 4, size_mult * 4))\n",
    "        x1 = tf.layers.batch_normalization(x1, training=training)\n",
    "        x1 = tf.maximum(alpha * x1, x1)\n",
    "        \n",
    "        x2 = tf.layers.conv2d_transpose(x1, size_mult * 2, 5, strides=2, padding='same')\n",
    "        x2 = tf.layers.batch_normalization(x2, training=training)\n",
    "        x2 = tf.maximum(alpha * x2, x2)\n",
    "        \n",
    "        x3 = tf.layers.conv2d_transpose(x2, size_mult, 5, strides=2, padding='same')\n",
    "        x3 = tf.layers.batch_normalization(x3, training=training)\n",
    "        x3 = tf.maximum(alpha * x3, x3)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.conv2d_transpose(x3, output_dim, 5, strides=2, padding='same')\n",
    "        \n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eieGn17nSbAL"
   },
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False, alpha=0.2, drop_rate=0., num_classes=10, size_mult=64):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        x = tf.layers.dropout(x, rate=drop_rate/2.5)\n",
    "        \n",
    "        # Input layer is ?x32x32x3\n",
    "        x1 = tf.layers.conv2d(x, size_mult, 3, strides=2, padding='same')\n",
    "        relu1 = tf.maximum(alpha * x1, x1)\n",
    "        relu1 = tf.layers.dropout(relu1, rate=drop_rate) # [?x16x16x?]\n",
    "\n",
    "        x2 = tf.layers.conv2d(relu1, size_mult, 3, strides=2, padding='same')\n",
    "        bn2 = tf.layers.batch_normalization(x2, training=True) # [?x8x8x?]\n",
    "        relu2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        x3 = tf.layers.conv2d(relu2, size_mult, 3, strides=2, padding='same') # [?x4x4x?]\n",
    "        bn3 = tf.layers.batch_normalization(x3, training=True)\n",
    "        relu3 = tf.maximum(alpha * bn3, bn3)\n",
    "        relu3 = tf.layers.dropout(relu3, rate=drop_rate)\n",
    "        \n",
    "        x4 = tf.layers.conv2d(relu3, 2 * size_mult, 3, strides=1, padding='same') # [?x4x4x?]\n",
    "        bn4 = tf.layers.batch_normalization(x4, training=True)\n",
    "        relu4 = tf.maximum(alpha * bn4, bn4)\n",
    "        \n",
    "        x5 = tf.layers.conv2d(relu4, 2 * size_mult, 3, strides=1, padding='same') # [?x4x4x?]\n",
    "        bn5 = tf.layers.batch_normalization(x5, training=True)\n",
    "        relu5 = tf.maximum(alpha * bn5, bn5)\n",
    "\n",
    "        x6 = tf.layers.conv2d(relu5, 2 * size_mult, 3, strides=2, padding='same') # [?x2x2x?]\n",
    "        bn6 = tf.layers.batch_normalization(x6, training=True)\n",
    "        relu6 = tf.maximum(alpha * bn6, bn6)\n",
    "        relu6 = tf.layers.dropout(relu6, rate=drop_rate)\n",
    "        \n",
    "        x7 = tf.layers.conv2d(relu5, filters=(2 * size_mult), kernel_size=3, strides=1, padding='valid')\n",
    "        # Don't use bn on this layer, because bn would set the mean of each feature\n",
    "        # to the bn mu parameter.\n",
    "        # This layer is used for the feature matching loss, which only works if\n",
    "        # the means can be different when the discriminator is run on the data than\n",
    "        # when the discriminator is run on the generator samples.\n",
    "        relu7 = tf.maximum(alpha * x7, x7)\n",
    "        \n",
    "        # Flatten it by global average pooling\n",
    "        # In global average pooling, for every feature map we take the average over all the spatial\n",
    "        # domain and return a single value\n",
    "        # In: [BATCH_SIZE,HEIGHT X WIDTH X CHANNELS] --> [BATCH_SIZE, CHANNELS]\n",
    "        features = tf.reduce_mean(relu7, axis=[1,2])\n",
    "        \n",
    "        # Set class_logits to be the inputs to a softmax distribution over the different classes\n",
    "        class_logits = tf.layers.dense(features, num_classes)\n",
    "        \n",
    "        # This function is more numerically stable than log(sum(exp(input))). \n",
    "        # It avoids overflows caused by taking the exp of large inputs and underflows \n",
    "        # caused by taking the log of small inputs.\n",
    "        gan_logits = tf.reduce_logsumexp(class_logits, 1)\n",
    "        \n",
    "        # Get the probability that the input is real rather than fake\n",
    "        out = tf.nn.softmax(class_logits) # class probabilities for the 10 real classes plus the fake class\n",
    "        \n",
    "        return out, class_logits, gan_logits, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ozH3-p3mSgdC"
   },
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, output_dim, y, num_classes, label_mask, alpha=0.2, drop_rate=0., smooth=0.1):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param input_real: Images from the real dataset\n",
    "    :param input_z: Z input random noise vector \n",
    "    :param output_dim: The number of channels in the output image\n",
    "    :param y: Integer class labels\n",
    "    :param num_classes: The number of classes\n",
    "    :param alpha: The slope of the left half of leaky ReLU activation\n",
    "    :param drop_rate: The probability of dropping a hidden unit\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # These numbers multiply the size of each layer of the generator and the discriminator,\n",
    "    # respectively. You can reduce them to run your code faster for debugging purposes.\n",
    "    g_size_mult = 32\n",
    "    d_size_mult = 64\n",
    "    \n",
    "    # Here we instatiate the generator and the discriminator networks\n",
    "    g_model = generator(input_z, output_dim, alpha=alpha, size_mult=g_size_mult)\n",
    "    d_on_data = discriminator(input_real, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)\n",
    "    \n",
    "    # d_model_real: probability that the input is real\n",
    "    # class_logits_on_data: the unnormalized log probability values for the probability of each classe\n",
    "    # gan_logits_on_data: the probability of whether or not the image is real\n",
    "    # data_features: features from the last layer of the discriminator to be used in the feature matching loss\n",
    "    d_model_real, class_logits_on_data, gan_logits_on_data, data_features = d_on_data\n",
    "    \n",
    "    d_on_samples = discriminator(g_model, reuse=True, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)\n",
    "    d_model_fake, class_logits_on_samples, gan_logits_on_samples, sample_features = d_on_samples\n",
    "    \n",
    "    # Here we compute `d_loss`, the loss for the discriminator.\n",
    "    # This should combine two different losses:\n",
    "    # 1. The loss for the GAN problem, where we minimize the cross-entropy for the binary\n",
    "    #    real-vs-fake classification problem.\n",
    "    real_data_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits_on_data, \n",
    "                                                            labels=tf.ones_like(gan_logits_on_data) * (1 - smooth)))\n",
    "    \n",
    "    fake_data_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits_on_samples, \n",
    "                                                         labels=tf.zeros_like(gan_logits_on_samples)))\n",
    "    \n",
    "    unsupervised_loss = real_data_loss + fake_data_loss\n",
    "    \n",
    "    #  2. The loss for the SVHN digit classification problem, where we minimize the cross-entropy\n",
    "    #     for the multi-class softmax. For this one we use the labels. Don't forget to ignore\n",
    "    #     use `label_mask` to ignore the examples that we are pretending are unlabeled for the\n",
    "    #     semi-supervised learning problem.\n",
    "    y = tf.squeeze(y)\n",
    "    suppervised_loss = tf.nn.softmax_cross_entropy_with_logits(logits=class_logits_on_data,\n",
    "                                                                  labels=tf.one_hot(y, num_classes, dtype=tf.float32))\n",
    "    \n",
    "    label_mask = tf.squeeze(tf.to_float(label_mask))\n",
    "    \n",
    "    # ignore the labels that we pretend does not exist for the loss\n",
    "    suppervised_loss = tf.reduce_sum(tf.multiply(suppervised_loss, label_mask))\n",
    "    \n",
    "    # get the mean \n",
    "    suppervised_loss = suppervised_loss / tf.maximum(1.0, tf.reduce_sum(label_mask))\n",
    "    d_loss = unsupervised_loss + suppervised_loss\n",
    "    \n",
    "    # Here we set `g_loss` to the \"feature matching\" loss invented by Tim Salimans at OpenAI.\n",
    "    # This loss consists of minimizing the absolute difference between the expected features\n",
    "    # on the data and the expected features on the generated samples.\n",
    "    # This loss works better for semi-supervised learning than the tradition GAN losses.\n",
    "    \n",
    "    # Make the Generator output features that are on average similar to the features \n",
    "    # that are found by applying the real data to the discriminator\n",
    "    \n",
    "    data_moments = tf.reduce_mean(data_features, axis=0)\n",
    "    sample_moments = tf.reduce_mean(sample_features, axis=0)\n",
    "    g_loss = tf.reduce_mean(tf.abs(data_moments - sample_moments))\n",
    "\n",
    "    pred_class = tf.cast(tf.argmax(class_logits_on_data, 1), tf.int32)\n",
    "    eq = tf.equal(tf.squeeze(y), pred_class)\n",
    "    correct = tf.reduce_sum(tf.to_float(eq))\n",
    "    masked_correct = tf.reduce_sum(label_mask * tf.to_float(eq))\n",
    "    \n",
    "    return d_loss, g_loss, correct, masked_correct, g_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0fLHlmYTEFA"
   },
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator loss Tensor\n",
    "    :param g_loss: Generator loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get weights and biases to update. Get them separately for the discriminator and the generator\n",
    "    discriminator_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES , scope='discriminator')\n",
    "    generator_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "    \n",
    "    # Minimize both players' costs simultaneously\n",
    "    #update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    #with tf.control_dependencies(update_ops):\n",
    "    d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1, name='d_optimizer').minimize(d_loss, var_list=discriminator_train_vars)\n",
    "    g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1, name='g_optimizer').minimize(g_loss, var_list=generator_train_vars)\n",
    "\n",
    "    shrink_lr = tf.assign(learning_rate, learning_rate * 0.9)\n",
    "    \n",
    "    return d_train_opt, g_train_opt, shrink_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufrzx32rTLDS"
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    \"\"\"\n",
    "    A GAN model.\n",
    "    :param real_size: The shape of the real data.\n",
    "    :param z_size: The number of entries in the z code vector.\n",
    "    :param learnin_rate: The learning rate to use for Adam.\n",
    "    :param num_classes: The number of classes to recognize.\n",
    "    :param alpha: The slope of the left half of the leaky ReLU activation\n",
    "    :param beta1: The beta1 parameter for Adam.\n",
    "    \"\"\"\n",
    "    def __init__(self, real_size, z_size, learning_rate, num_classes=10, alpha=0.2, beta1=0.5):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.learning_rate = tf.Variable(learning_rate, trainable=False)\n",
    "        inputs = model_inputs(real_size, z_size)\n",
    "        self.input_real, self.input_z, self.y, self.label_mask = inputs\n",
    "        self.drop_rate = tf.placeholder_with_default(.5, (), \"drop_rate\")\n",
    "        \n",
    "        loss_results = model_loss(self.input_real, self.input_z,\n",
    "                                  real_size[2], self.y, num_classes,\n",
    "                                  label_mask=self.label_mask,\n",
    "                                  alpha=0.2,\n",
    "                                  drop_rate=self.drop_rate)\n",
    "        self.d_loss, self.g_loss, self.correct, self.masked_correct, self.samples = loss_results\n",
    "        \n",
    "        self.d_opt, self.g_opt, self.shrink_lr = model_opt(self.d_loss, self.g_loss, self.learning_rate, beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITGR11qQTRq4"
   },
   "outputs": [],
   "source": [
    "def view_samples(epoch, samples, nrows, ncols, figsize=(5,5)):\n",
    "    fig, axes = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols, \n",
    "                             sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        ax.axis('off')\n",
    "        img = ((img - img.min())*255 / (img.max() - img.min())).astype(np.uint8)\n",
    "        ax.set_adjustable('box-forced')\n",
    "        im = ax.imshow(img)\n",
    "   \n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcAMkujbTWRy"
   },
   "outputs": [],
   "source": [
    "def train(net, dataset, epochs, batch_size, figsize=(5,5)):\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    sample_z = np.random.normal(0, 1, size=(50, z_size))\n",
    "\n",
    "    samples, train_accuracies, test_accuracies = [], [], []\n",
    "    steps = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for e in range(epochs):\n",
    "            print(\"Epoch\",e)\n",
    "            \n",
    "            t1e = time.time()\n",
    "            num_examples = 0\n",
    "            num_correct = 0\n",
    "            for x, y, label_mask in dataset.batches(batch_size):\n",
    "                assert 'int' in str(y.dtype)\n",
    "                steps += 1\n",
    "                num_examples += label_mask.sum()\n",
    "\n",
    "                # Sample random noise for G\n",
    "                batch_z = np.random.normal(0, 1, size=(batch_size, z_size))\n",
    "\n",
    "                # Run optimizers\n",
    "                t1 = time.time()\n",
    "                _, _, correct = sess.run([net.d_opt, net.g_opt, net.masked_correct],\n",
    "                                         feed_dict={net.input_real: x, net.input_z: batch_z,\n",
    "                                                    net.y : y, net.label_mask : label_mask})\n",
    "                t2 = time.time()\n",
    "                num_correct += correct\n",
    "\n",
    "            sess.run([net.shrink_lr])\n",
    "            \n",
    "            \n",
    "            train_accuracy = num_correct / float(num_examples)\n",
    "            \n",
    "            print(\"\\t\\tClassifier train accuracy: \", train_accuracy)\n",
    "            \n",
    "            num_examples = 0\n",
    "            num_correct = 0\n",
    "            for x, y in dataset.batches(batch_size, which_set=\"test\"):\n",
    "                assert 'int' in str(y.dtype)\n",
    "                num_examples += x.shape[0]\n",
    "\n",
    "                correct, = sess.run([net.correct], feed_dict={net.input_real: x,\n",
    "                                                   net.y : y,\n",
    "                                                   net.drop_rate: 0.})\n",
    "                num_correct += correct\n",
    "            \n",
    "            test_accuracy = num_correct / float(num_examples)\n",
    "            print(\"\\t\\tClassifier test accuracy\", test_accuracy)\n",
    "            print(\"\\t\\tStep time: \", t2 - t1)\n",
    "            t2e = time.time()\n",
    "            print(\"\\t\\tEpoch time: \", t2e - t1e)\n",
    "            \n",
    "            \n",
    "            gen_samples = sess.run(\n",
    "                                   net.samples,\n",
    "                                   feed_dict={net.input_z: sample_z})\n",
    "            samples.append(gen_samples)\n",
    "            _ = view_samples(-1, samples, 5, 10, figsize=figsize)\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            # Save history of accuracies to view after training\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            \n",
    "\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "\n",
    "    with open('samples.pkl', 'wb') as f:\n",
    "        pkl.dump(samples, f)\n",
    "    \n",
    "    return train_accuracies, test_accuracies, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W2rzC55xTdTL",
    "outputId": "21ea3070-ec64-4c26-b7b5-36ff1ff428ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: checkpoints: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "6edlM9hPTj0v",
    "outputId": "fe1cb917-4e15-4c08-c5b5-21d0402dd0af"
   },
   "outputs": [],
   "source": [
    "real_size = (28,28,1)\n",
    "z_size = 100\n",
    "learning_rate = 0.0003\n",
    "\n",
    "net = GAN(real_size, z_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xd8zUHJHTuxn"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(trainX,trainY, testX,testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x6495efb10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cT_FBYWiXrkJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 47754 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-8adf9ec096f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                    \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                    \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                                                    )\n",
      "\u001b[0;32m<ipython-input-43-d1535d77aff2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, dataset, epochs, batch_size, figsize)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_mask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-5c111d81ab3d>\u001b[0m in \u001b[0;36mbatches\u001b[0;34m(self, batch_size, which_set)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwhich_set\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 47754 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "train_accuracies, test_accuracies, samples = train(net,\n",
    "                                                   dataset,\n",
    "                                                   epochs,\n",
    "                                                   batch_size\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sgan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
